{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Settings\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from load_data import load_process_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"SkinCancer/Thesis_Dataset/training.pl\"\n",
    "models_folder = root_folder + \"Models/Bi-LSTM/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/training.pl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2017-05-21 15:57:46.565383\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_words(tessays):\n",
    "    wds = set()\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                wds.add(word)\n",
    "    return wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|TD|=1552 |VD|=1170 |diff|=30\n",
      "|TD|=1567 |VD|=1179 |diff|=15\n",
      "|TD|=1563 |VD|=1155 |diff|=19\n",
      "|TD|=1570 |VD|=1122 |diff|=12\n",
      "|TD|=1549 |VD|=1191 |diff|=33\n",
      "\n",
      "0.01873818119305484\n",
      "\n",
      "CPU times: user 154 ms, sys: 2.93 ms, total: 157 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folds = cross_validation(tagged_essays, CV_FOLDS)\n",
    "diff_counts = []\n",
    "vd_lens = []\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    td_wds = get_unique_words(essays_TD)\n",
    "    vd_wds = get_unique_words(essays_VD)\n",
    "    diff = vd_wds - td_wds\n",
    "    diff_counts.append(len(diff))\n",
    "    vd_lens.append(len(vd_wds))\n",
    "    print(\"|TD|={td_len} |VD|={vd_len} |diff|={len_diff}\".format(td_len=len(td_wds), vd_len=len(vd_wds), len_diff=len(diff)))\n",
    "    \n",
    "print(\"\")\n",
    "print(float(sum(diff_counts)) / sum(vd_lens))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 files found\n",
      "218 essays processed\n"
     ]
    }
   ],
   "source": [
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "test_config = get_config(test_folder)\n",
    "test_essays = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Test/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|TD|=1582 |VD|=899 |diff|=41\n",
      "0.04560622914349277\n"
     ]
    }
   ],
   "source": [
    "td_wds = get_unique_words(tagged_essays)\n",
    "vd_wds = get_unique_words(test_essays)\n",
    "diff = vd_wds - td_wds\n",
    "print(\"|TD|={td_len} |VD|={vd_len} |diff|={len_diff}\".format(td_len=len(td_wds), vd_len=len(vd_wds), len_diff=len(diff)))\n",
    "print(float(len(diff))/len(vd_wds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
