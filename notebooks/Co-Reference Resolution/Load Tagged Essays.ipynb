{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from Rpfa import micro_rpfa\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902 files found\n",
      "902 essays processed\n"
     ]
    }
   ],
   "source": [
    "config = get_config(training_folder)\n",
    "# override this so we don't replace INFREQUENT words\n",
    "config[\"min_df\"] = 0\n",
    "tagged_essays2 = load_process_essays(**config)\n",
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_essays2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Training'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coref_folder = root_folder + \"CoReference/Training_Old\"\n",
    "coref_folder = root_folder + \"CoReference/Training\"\n",
    "coref_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from FindFiles import find_files\n",
    "coref_files = find_files(coref_folder, \".*\\.tagged\")\n",
    "len(coref_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902\n"
     ]
    }
   ],
   "source": [
    "def parse_stanfordnlp_tagged_essays(coref_files):\n",
    "    DELIM = \"->\"\n",
    "    DELIM_TAG = \"|||\"\n",
    "\n",
    "    essay2tagged = {}\n",
    "    for fname in coref_files:\n",
    "        with open(fname) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        tagged_lines = []\n",
    "        for line in lines:\n",
    "            tagged_words = []\n",
    "            line = line.strip()\n",
    "            wds = []\n",
    "            for t_token in line.split(\" \"):\n",
    "                ##print(t_token)\n",
    "\n",
    "                word, tags = t_token.split(DELIM)\n",
    "                if word == \"-lrb-\":\n",
    "                    word = \"(\"\n",
    "                if word == \"-rrb-\":\n",
    "                    word = \")\"\n",
    "                wds.append(word)\n",
    "                tag_dict = {}\n",
    "                for tag in tags.split(DELIM_TAG):\n",
    "                    if not tag:\n",
    "                        continue\n",
    "                    splt = tag.split(\":\")\n",
    "                    if len(splt) == 2:\n",
    "                        key, val = splt\n",
    "                        tag_dict[key] = val\n",
    "                    else:\n",
    "                        raise Exception(\"Error\")\n",
    "                tagged_words.append((word, tag_dict))\n",
    "            tagged_lines.append(tagged_words)\n",
    "        essay2tagged[fname.split(\"/\")[-1].split(\".\")[0]] = tagged_lines\n",
    "    return essay2tagged\n",
    "\n",
    "essay2tagged = parse_stanfordnlp_tagged_essays(coref_files)\n",
    "print(len(essay2tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5125, 5125, 5125)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = []\n",
    "mention_lens = []\n",
    "\n",
    "current_ph_len = 0\n",
    "current_mention = \"\"\n",
    "prev_phrase = \"\"\n",
    "\n",
    "mention2replace = []\n",
    "\n",
    "def change_in_phrase():\n",
    "    global mentions, mention_lens, current_ph_len, current_mention, prev_phrase, mention2replace \n",
    "    if current_ph_len > 0:\n",
    "        mention_lens.append(current_ph_len)\n",
    "        mentions.append(current_mention.strip())\n",
    "        mention2replace.append((current_mention.strip(), prev_phrase))\n",
    "    current_mention = \"\"\n",
    "    current_ph_len = 0\n",
    "    prev_phrase = \"\"\n",
    "\n",
    "for ename, tagged_lines in essay2tagged.items():\n",
    "    #print(ename)\n",
    "    for line in tagged_lines:\n",
    "        current_ph_len = 0\n",
    "        current_mention = \"\"\n",
    "        prev_phrase = \"\"\n",
    "        for wd, tag_dict in line:\n",
    "            if \"COREF_PHRASE\" in tag_dict:\n",
    "                current_phrase = tag_dict[\"COREF_PHRASE\"]\n",
    "                if prev_phrase != current_phrase:\n",
    "                    change_in_phrase()\n",
    "                current_ph_len+=1\n",
    "                current_mention += \" \" + wd\n",
    "                prev_phrase = tag_dict[\"COREF_PHRASE\"]\n",
    "            else:\n",
    "                change_in_phrase()\n",
    "        change_in_phrase()\n",
    "len(mention_lens), len(mentions), len(mention2replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5473170731707317, 20, 1)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(mention_lens), np.max(mention_lens), np.min(mention_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['producers ( things that create food using sunlight like coral reef )',\n",
       " 'the salty water',\n",
       " 'the pacific ocean',\n",
       " 'corals living in the oceans',\n",
       " 'the atlantic ocean',\n",
       " 'the amount of carbon dioxide',\n",
       " 'the amount of co2',\n",
       " 'the coral reef',\n",
       " 'the amount of fresh water',\n",
       " 'the zooxanthallae algae']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in mentions if len(m.strip().split(\" \")) >= 3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the winds',\n",
       "  'the_trade_winds_that_go_through_upwelling_which_can_increase_the_temperature_from_3of_to_5of_and_in_some_places_they_increase_over_10of'),\n",
       " ('the trade winds',\n",
       "  'the_trade_winds_that_go_through_upwelling_which_can_increase_the_temperature_from_3of_to_5of_and_in_some_places_they_increase_over_10of'),\n",
       " ('the photosynthesis',\n",
       "  'the_photosynthesis_the_polyps_go_through_to_recieve_energy_to_give_to_the_coral_which_they_need_00_%_to_00_%_of'),\n",
       " ('the ways',\n",
       "  'many_ways_to_explain_the_rates_in_coral_bleaching_like_trade_winds_,_a_balanced_environment_,_and_physical_damage'),\n",
       " ('it',\n",
       "  'the_change_in_carbon_dioxide_which_is_a_cause_of_different_temperatures_and_extreme_storms_coral_how_salty_the_water')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(mention2replace, key = lambda s: -len(s[1].split(\"_\")))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m,h in mention2replace:\n",
    "#     if len(m.split(\" \")) > len(h.split(\"_\")):\n",
    "#         print(m)\n",
    "#         print(h)\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 902)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay2parsed = {}\n",
    "for e in tagged_essays2:\n",
    "    essay2parsed[e.name.split(\".\")[0]] = e\n",
    "len(essay2parsed), len(essay2tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(essay2parsed.keys())\n",
    "b = set(essay2tagged.keys())\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR |the algae|||\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_cnt = 0\n",
    "COREF_PHRASE = \"COREF_PHRASE\"\n",
    "SCAN_LENGTH = 4\n",
    "\n",
    "replacements = []\n",
    "fuzzy_matches = []\n",
    "\n",
    "for ename, tagged_essay in essay2tagged.items():\n",
    "    assert ename in essay2parsed\n",
    "    essay = essay2parsed[ename]\n",
    "    \n",
    "    wds1 = []\n",
    "    taggedwd2sentixs = {}\n",
    "    for sent_ix,sent in enumerate(essay.sentences):\n",
    "        for wd_ix, (wd,tags) in enumerate(sent):\n",
    "            taggedwd2sentixs[len(wds1)] = (sent_ix, wd_ix)\n",
    "            wds1.append((wd,tags))            \n",
    "\n",
    "    wds2 = []\n",
    "    mentions = []\n",
    "    for sent_ix, sent in enumerate(tagged_essay):\n",
    "        current_mention = \"\"\n",
    "        mention_ixs = set()\n",
    "        for wd_ix, (wd,tag_dict) in enumerate(sent):\n",
    "            wds2.append((wd,tag_dict))\n",
    "            if COREF_PHRASE not in tag_dict:\n",
    "                if current_mention != \"\":\n",
    "                    mentions.append((current_mention, mention_ixs))\n",
    "                current_mention = \"\"\n",
    "                mention_ixs = set()\n",
    "            else:\n",
    "                phrase = tag_dict[COREF_PHRASE].replace(\"_\",\" \")\n",
    "                if phrase != current_mention and current_mention != \"\":\n",
    "                    mentions.append((current_mention, mention_ixs))\n",
    "                    current_mention = \"\"\n",
    "                    mention_ixs = set()\n",
    "                current_mention = phrase\n",
    "                mention_ixs.add(len(wds2)-1)                \n",
    "        if current_mention != \"\":\n",
    "            mentions.append((current_mention, mention_ixs))\n",
    "    \n",
    "    if len(mentions) == 0:\n",
    "        continue\n",
    "        \n",
    "    ix_a, ix_b = 0,0\n",
    "    wd1ix_wd2ix = {}\n",
    "    while ix_a < (len(wds1)-1) and ix_b < (len(wds2)-1):\n",
    "        a, atags = wds1[ix_a]\n",
    "        b, btag_dict = wds2[ix_b]\n",
    "\n",
    "        if a == b:\n",
    "            wd1ix_wd2ix[ix_a] = ix_b\n",
    "            ix_a += 1\n",
    "            ix_b += 1\n",
    "        else:\n",
    "            # look ahead in wds2 for item that matches next a\n",
    "            found_match = False\n",
    "            for offseta, (aa,atags) in enumerate(wds1[ix_a: ix_a+1+SCAN_LENGTH]):\n",
    "                for offsetb, (bb,bb_tag_dict) in enumerate(wds2[ix_b:ix_b+1+SCAN_LENGTH]):\n",
    "                    if aa == bb:\n",
    "                        ix_a = ix_a + offseta\n",
    "                        ix_b = ix_b + offsetb\n",
    "                        wd1ix_wd2ix[ix_a] = ix_b\n",
    "                        found_match = True\n",
    "                        break\n",
    "                if found_match:\n",
    "                    break\n",
    "            if not found_match:                \n",
    "                print(\"Failed: \" + ename, a, b, ix_a, len(wds1), ix_b, len(wds2))\n",
    "                failed_cnt +=1\n",
    "                break\n",
    "    \n",
    "    for mention, ixs in mentions:\n",
    "        first_ix = min(ixs)\n",
    "        is_fuzzy = False\n",
    "        if first_ix not in wd1ix_wd2ix:\n",
    "            while first_ix > 0 and first_ix not in wd1ix_wd2ix:\n",
    "                first_ix -= 1\n",
    "            if first_ix not in wd1ix_wd2ix:\n",
    "                e_first_wd_ix = 0\n",
    "            # one past last matching index\n",
    "            else:\n",
    "                e_first_wd_ix = min(len(wds1)-1,wd1ix_wd2ix[first_ix]+1)\n",
    "            is_fuzzy = True\n",
    "        else:\n",
    "            e_first_wd_ix = wd1ix_wd2ix[first_ix]\n",
    "\n",
    "        last_ix = max(ixs)\n",
    "        if last_ix not in wd1ix_wd2ix:\n",
    "            while last_ix < len(wds2) and last_ix not in wd1ix_wd2ix:\n",
    "                last_ix += 1\n",
    "            if last_ix not in wd1ix_wd2ix:\n",
    "                e_last_wd_ix = len(wds1) -1\n",
    "            else:\n",
    "                e_last_wd_ix = max(0,wd1ix_wd2ix[last_ix]-1)\n",
    "            is_fuzzy = True\n",
    "        else:\n",
    "            e_last_wd_ix = wd1ix_wd2ix[last_ix]\n",
    "\n",
    "        replacement = []\n",
    "        \n",
    "        for e_wd_ix in range(e_first_wd_ix,e_last_wd_ix+1):                        \n",
    "            sent_ix, sent_wd_ix = taggedwd2sentixs[e_wd_ix]\n",
    "            sentence = essay.sentences[sent_ix]\n",
    "            wd, tags = sentence[sent_wd_ix]\n",
    "            replacement.append((wd, tags))\n",
    "        \n",
    "        if replacement:\n",
    "            replacements.append((mention, replacement))\n",
    "            if is_fuzzy:\n",
    "                fuzzy_matches.append((mention, replacement))\n",
    "\n",
    "        if len(replacement) < (len(ixs)/2.0):\n",
    "        #if len(replacement) == 0:\n",
    "            print(\"ERROR\", \"|\" + mention + \"|||\")#, \" \".join(list(zip(*fuzzy_matches))[0]), len(replacement), len(ixs))\n",
    "        \n",
    "failed_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fuzzy_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the world coral reefs                              ||| reefs .\n",
      "coral and zooxathellae algae                       ||| algae .\n",
      "the coral                                          ||| .\n",
      "the water basin                                    ||| it\n",
      "of the relationship coral + algae                  ||| they'll eject\n",
      "algae                                              ||| the aldi\n",
      "algae                                              ||| the aldi\n",
      "algae                                              ||| the aldi\n",
      "the text '' coral                                  ||| in water\n",
      "the text '' coral                                  ||| algae will\n",
      "the text '' coral                                  ||| coral and zooxanthallae\n",
      "some coral                                         ||| the coral\n",
      "corals                                             ||| .\n",
      "they                                               ||| them\n"
     ]
    }
   ],
   "source": [
    "for mention, replacement in fuzzy_matches:\n",
    "    wds = list(zip(*replacement))[0]\n",
    "    tags = list(zip(*replacement))[1]\n",
    "    print(mention.ljust(50), \"|||\", \" \".join(wds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bleached coral', {53}),\n",
       " ('bleached coral', {93, 94}),\n",
       " ('the zooxanthellae algae', {103, 104}),\n",
       " ('bleached coral', {110, 111}),\n",
       " ('most zooxanthellae', {113, 114}),\n",
       " ('most zooxanthellae', {120}),\n",
       " ('this relationship', {131}),\n",
       " ('bleached coral', {152, 153})]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it set() bleached coral\n",
      "provides set() bleached coral\n",
      "a set() bleached coral\n",
      "this set() the zooxanthellae algae\n",
      ". set() the zooxanthellae algae\n",
      ". set() bleached coral\n",
      "the set() bleached coral\n",
      "pass set() most zooxanthellae\n",
      "some set() most zooxanthellae\n",
      "from set() most zooxanthellae\n",
      "states set() this relationship\n",
      "and {'Causer:14->Result:50', 'Result', 'Causer', 'Causer:14', 'Causer:6->Result:14', '14', 'Result:14'} bleached coral\n",
      "zooxanthellae {'Causer:14->Result:50', 'Result', 'Causer', 'Causer:14', 'Causer:6->Result:14', '14', 'Result:14'} bleached coral\n"
     ]
    }
   ],
   "source": [
    "for mention, ixs in mentions:\n",
    "    for wd_ix in ixs:\n",
    "        e_wd_ix = wd1ix_wd2ix[wd_ix]\n",
    "        sent_ix, sent_wd_ix = taggedwd2sentixs[e_wd_ix]\n",
    "        sentence = essay.sentences[sent_ix]\n",
    "        wd, tags = sentence[sent_wd_ix]\n",
    "        print(wd, tags, mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07047872340425532"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([l for l in diff_lens if l > 2])/sum(diff_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1165553080920565, 4, 1.0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(diff_lens), np.max(diff_lens), np.median(diff_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a coral is a living thing that lives in the ocean , it known for their bright color .\n",
      "1 the reason why they call it coral bleaching is because when it bleached it loses it color and becomes plain white stated in the article \" background : what is \" coral bleaching \" . \"\n",
      "2 there different explanations to what leads to differences in the rates of coral bleaching .\n",
      "3 one of these reasons is due to the trade wind happening in the ocean .\n",
      "4 when the trade winds reverse it cause the water temperature to change .\n",
      "5 due to this movement regions start to swell causing the sea - levels to rise .\n",
      "6 the article \" shifting trade winds \" it states that it more affected in the pacific ocean .\n",
      "7 as stated in the article \" what is coral bleaching \" saying that the pacific ocean is where coral bleaching is more done .\n",
      "8 another reason that explains the rate of coral bleaching is the zooxanthellage which has a symbiotic relationship with coral .\n",
      "9 when the coral happens to be bleached the zooxanthellae gets affected and makes the coral die easier since it is a living thing , based inthe article \" coral and zooxanthellae . \"\n"
     ]
    }
   ],
   "source": [
    "e = essay2parsed[\"EBA1415_KYNS_3_CB_ES-05384\"]\n",
    "for sent_ix, sent in enumerate(e.sentences):\n",
    "    wds = []\n",
    "    for wd,tags in sent:\n",
    "        wds.append(wd)\n",
    "    print(sent_ix, \" \".join(wds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a coral is a living thing that lives in the ocean , it known for their bright color .\n",
      "1 the reason why they call it coral bleaching is because when it bleached it loses it color and becomes plain white stated in the article '' background : what is '' coral bleaching '' . ''\n",
      "2 there different explanations to what leads to differences in the rates of coral bleaching .\n",
      "3 one of these reasons is due to the trade wind happening in the ocean .\n",
      "4 when the trade winds reverse it cause the water temperature to change .\n",
      "5 due to this movement regions start to swell causing the sea - levels to rise .\n",
      "6 the article '' shifting trade winds '' it states that it more affected in the pacific ocean .\n",
      "7 as stated in the article '' what is coral bleaching '' saying that the pacific ocean is where coral bleaching is more done .\n",
      "8 another reason that explains the rate of coral bleaching is the zooxanthellage which has a symbiotic relationship with coral .\n",
      "9 when the coral happens to be bleached the zooxanthellae gets affected and makes the coral die easier since it is a living thing , based inthe article '' coral and zooxanthellae . ''\n"
     ]
    }
   ],
   "source": [
    "e = essay2tagged[\"EBA1415_KYNS_3_CB_ES-05384\"]\n",
    "for sent_ix, sent in enumerate(e):\n",
    "    wds = []\n",
    "    for wd,tags in sent:\n",
    "        wds.append(wd)\n",
    "    print(sent_ix, \" \".join(wds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ename, tagged_essay in essay2tagged.items():\n",
    "    assert ename in essay2parsed\n",
    "    essay = essay2parsed[ename]\n",
    "         \n",
    "    wds1 = []\n",
    "    ix2_wd_sent_ix = {}\n",
    "    ix = 0\n",
    "    for sent_ix, sent in enumerate(essay.sentences):\n",
    "        for wd_ix, (wd,tags) in enumerate(sent):\n",
    "            ix2_wd_sent_ix[len(wds1)] = (sent_ix, wd_ix)\n",
    "            wds1.append(wd)\n",
    "\n",
    "    wds2 = []\n",
    "    for sent in tagged_essay:\n",
    "        for wd,tag_dict in sent:\n",
    "            if \"COREF_PHRASE\" in tag_dict:\n",
    "                #print(\"Found\")\n",
    "                wds2.append(wd)\n",
    "            \n",
    "    ix_a, ix_b = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
