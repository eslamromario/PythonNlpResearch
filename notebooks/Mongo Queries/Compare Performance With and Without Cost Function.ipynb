{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_coll = 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_COST_FN_VD'\n",
    "sc_coll = 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_COST_FN_VD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from bson.son import SON # needed to ensure dictionary is ordered (python default is not)\n",
    "\n",
    "feats_pipeline = [{\n",
    "    \"$project\": { \n",
    "            \"weighted_f1_score\":\"$WEIGHTED_MEAN_CONCEPT_CODES.f1_score\",\n",
    "            \"micro_f1_score\":  \"$MICRO_F1.f1_score\",\n",
    "            \"micro_recall\":    \"$MICRO_F1.recall\",\n",
    "            \"micro_precision\": \"$MICRO_F1.precision\",\n",
    "            \"stemmed\":        \"$parameters.stemmed\",\n",
    "            \"num_feats\":      \"$parameters.num_feats_MEAN\",\n",
    "            \"feats\":          \"$parameters.extractors\",\n",
    "            \"cost_fn\":        \"$parameters.cost_function\",\n",
    "            \"count\": {        \"$size\" : \"$parameters.extractors\" },\n",
    "            \"asof\" :          \"$asof\",\n",
    "            \"_id\":1\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"$match\":{\n",
    "        \"micro_f1_score\": { \"$exists\" : True },\n",
    "        # how many feats\n",
    "        #\"count\": {          \"$eq\" :1 },\n",
    "        # window width\n",
    "        #\"window_size\": {    \"$eq\":13 }\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"$sort\":{\n",
    "        #\"weighted_f1_score\":-1,\n",
    "        \"micro_f1_score\": -1\n",
    "        #\"asof\": -1\n",
    "        #\"count\": -1\n",
    "    }\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_df_sorted_by_f1score(rows):\n",
    "    df = pd.DataFrame(rows).sort_values(\"micro_f1_score\", ascending=False)\n",
    "    return df\n",
    "\n",
    "cb_vd_df = get_df_sorted_by_f1score(cb_vd_rows)\n",
    "sc_vd_df = get_df_sorted_by_f1score(sc_vd_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coral Bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_COST_FN_VD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>micro_f1_score</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>cost_fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.726334</td>\n",
       "      <td>0.704508</td>\n",
       "      <td>0.749556</td>\n",
       "      <td>38096.0</td>\n",
       "      <td>micro_f1_cost_plusepsilon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.694097</td>\n",
       "      <td>0.647746</td>\n",
       "      <td>0.747592</td>\n",
       "      <td>27500.0</td>\n",
       "      <td>uniform_cost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  micro_f1_score  micro_recall  micro_precision  num_feats  \\\n",
       "0      6        0.726334      0.704508         0.749556    38096.0   \n",
       "1      6        0.694097      0.647746         0.747592    27500.0   \n",
       "\n",
       "                     cost_fn  \n",
       "0  micro_f1_cost_plusepsilon  \n",
       "1               uniform_cost  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = cb_coll\n",
    "print(coll)\n",
    "rows = [row for row in db[coll].aggregate(feats_pipeline)]\n",
    "df = get_df_sorted_by_f1score(rows)\n",
    "cols = \"count,micro_f1_score,micro_recall,micro_precision,num_feats,cost_fn\".split(\",\")\n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_COST_FN_VD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>micro_f1_score</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>cost_fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.78785</td>\n",
       "      <td>0.754025</td>\n",
       "      <td>0.824853</td>\n",
       "      <td>28022.0</td>\n",
       "      <td>micro_f1_cost_plusepsilon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.78620</td>\n",
       "      <td>0.723614</td>\n",
       "      <td>0.860638</td>\n",
       "      <td>22586.0</td>\n",
       "      <td>uniform_cost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  micro_f1_score  micro_recall  micro_precision  num_feats  \\\n",
       "0      4         0.78785      0.754025         0.824853    28022.0   \n",
       "1      4         0.78620      0.723614         0.860638    22586.0   \n",
       "\n",
       "                     cost_fn  \n",
       "0  micro_f1_cost_plusepsilon  \n",
       "1               uniform_cost  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = sc_coll\n",
    "print(coll)\n",
    "rows = [row for row in db[coll].aggregate(feats_pipeline)]\n",
    "df = get_df_sorted_by_f1score(rows)\n",
    "cols = \"count,micro_f1_score,micro_recall,micro_precision,num_feats,cost_fn\".split(\",\")\n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Examples on Average Per Code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "data_directory =  \"/Users/simon.hughes/Google Drive/Phd/Data/\"\n",
    "root_folder = data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays_cb = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "226 files found\n",
      "226 essays processed\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_process_essays\n",
    "from window_based_tagger_config import get_config\n",
    "from Settings import Settings\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test_cb = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays_sc = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 files found\n",
      "218 essays processed\n"
     ]
    }
   ],
   "source": [
    "root_folder = settings.data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test_sc = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crel_helper import get_tag_freq\n",
    "\n",
    "tag_freq_train = get_tag_freq(pred_tagged_essays_cb, [])\n",
    "crel_tags_train = list((t for t in tag_freq_train.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                )\n",
    "                      )\n",
    "\n",
    "tag_freq_test = get_tag_freq(tagged_essays_test_cb, [])\n",
    "crel_tags_test = list((t for t in tag_freq_test.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                )\n",
    "                     )\n",
    "\n",
    "crels_cb = set(crel_tags_test).union(set(crel_tags_train))\n",
    "len(crels_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.990697674418605\n"
     ]
    }
   ],
   "source": [
    "coll = \"TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_TD\"\n",
    "\n",
    "for row in db[coll].find({}):\n",
    "    precision, recall,ncodes = [],[],[]        \n",
    "    for k in row.keys():\n",
    "        #TODO - fix this to be more specific\n",
    "        if k in crels_cb:\n",
    "            ncodes.append(row[k][\"num_codes\"])\n",
    "    print(np.mean(ncodes)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crel_helper import get_tag_freq\n",
    "\n",
    "tag_freq_train = get_tag_freq(pred_tagged_essays_sc, [])\n",
    "crel_tags_train = list((t for t in tag_freq_train.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                )\n",
    "                      )\n",
    "\n",
    "tag_freq_test = get_tag_freq(tagged_essays_test_sc, [])\n",
    "crel_tags_test = list((t for t in tag_freq_test.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                )\n",
    "                     )\n",
    "\n",
    "crels_sc = set(crel_tags_test).union(set(crel_tags_train))\n",
    "len(crels_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.30204081632653\n"
     ]
    }
   ],
   "source": [
    "coll = \"TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_TD\"\n",
    "\n",
    "for row in db[coll].find({}):\n",
    "    precision, recall,ncodes = [],[],[]        \n",
    "    for k in row.keys():\n",
    "        #TODO - fix this to be more specific\n",
    "        if k in crels_sc:\n",
    "            ncodes.append(row[k][\"num_codes\"])\n",
    "    print(np.mean(ncodes)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
