{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import pymongo\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by(df, bycols, agg_map):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df:      DataFrame\n",
    "    @param bycols:  str or list\n",
    "                        Column(s) to group by\n",
    "    @param agg_map: dictionary or list of 2-tuples\n",
    "                        Mapping from column to aggregate function e.g. [(\"city\", \"count\"), (\"salary\", \"mean\"]\n",
    "    @return:        DataFrame\n",
    "                        Flattened dataframe, with multi-level index removed\n",
    "    \"\"\"\n",
    "    grps = []\n",
    "    if type(bycols) == str:\n",
    "        bycols = [bycols]\n",
    "\n",
    "    if type(agg_map) == dict:\n",
    "        agg_map = agg_map.items()\n",
    "\n",
    "    for k,v in agg_map:\n",
    "        grp = df[bycols + [k]].groupby(bycols, ).agg(v)\n",
    "        grp.reset_index(inplace=True)\n",
    "        grp[\"%s(%s)\" % (v,k)] = grp[k]\n",
    "        del grp[k]\n",
    "        grps.append(grp)\n",
    "\n",
    "    m = grps[0]\n",
    "    for grp in grps[1:]:\n",
    "        m = pd.merge(m, grp, on=bycols, how=\"inner\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bson.son import SON # needed to ensure dictionary is ordered (python default is not)\n",
    "import hashlib\n",
    "\n",
    "def hash_feats(fts):\n",
    "    vals = fts.values\n",
    "    joined = \"|\".join(map(lambda s: str(s),vals)).encode('utf-8') \n",
    "    return hashlib.sha224(joined).hexdigest()\n",
    "\n",
    "def get_df_sorted_by_f1score(collection, params=None, filter_cols=True):\n",
    "    if not params:\n",
    "        params = []\n",
    "    if type(params) == str:\n",
    "        params = params.split(\",\")\n",
    "    \n",
    "    project = {\n",
    "            \"weighted_f1_score\":\"$WEIGHTED_MEAN_CONCEPT_CODES.f1_score\",\n",
    "            \"macro_f1_score\":   \"$MACRO_F1\",\n",
    "            \"micro_f1_score\":  \"$MICRO_F1.f1_score\",\n",
    "            \"micro_recall\":    \"$MICRO_F1.recall\",\n",
    "            \"micro_precision\": \"$MICRO_F1.precision\",\n",
    "    \n",
    "    # PARAMETERS            \n",
    "            \"window_size\":    \"$parameters.window_size\",\n",
    "            \"feats\":          \"$parameters.extractors\",\n",
    "            \"count\": {        \"$size\" : \"$parameters.extractors\" },\n",
    "            \"asof\" :          \"$asof\",\n",
    "            \"_id\":1\n",
    "    }\n",
    "    \n",
    "    # No count for HMM\n",
    "    if \"_hmm\" in collection.lower():\n",
    "        del project[\"count\"]\n",
    "    \n",
    "    for param in params:\n",
    "        project[param] = \"$parameters.\" + param\n",
    "\n",
    "    feats_pipeline = [{\n",
    "        \"$project\": project\n",
    "    },\n",
    "    {\n",
    "        \"$match\":{\n",
    "            \"micro_f1_score\": { \"$exists\" : True }        \n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\":{\n",
    "            \"micro_f1_score\": -1\n",
    "        }\n",
    "    },\n",
    "    ]\n",
    "    \n",
    "    rows = [row for row in db[collection].aggregate(feats_pipeline)]\n",
    "    df = pd.DataFrame(rows).sort_values(\"micro_f1_score\", ascending=False)\n",
    "    if params:\n",
    "        df[\"hs_params\"] = df[params].apply(hash_feats, axis=1)\n",
    "        \n",
    "    if filter_cols:\n",
    "        cols = [\"micro_f1_score\", \"micro_recall\" ,\"micro_precision\", \"macro_f1_score\" ] + params\n",
    "        return df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_window_classifier_results(prefix):\n",
    "    collections = \"WINDOW_CLASSIFIER_BR,WINDOW_CLASSIFIER_LBL_POWERSET_MULTICLASS,WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS\".split(\",\")\n",
    "    dfs = []\n",
    "    for c in collections:\n",
    "        col = prefix + c\n",
    "        print(col)\n",
    "        df = dict(get_df_sorted_by_f1score(col).iloc[0,:])\n",
    "        df[\"Collection_\" + prefix[:-1]] = col.replace(prefix,\"\")\n",
    "        dfs.append(df)\n",
    "    return pd.DataFrame(dfs).sort_values(\"micro_f1_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round_data(df, places=3):\n",
    "    df_copy = df.copy()\n",
    "    fmt_str = \"{0:.\" + str(places) + \"f}\"\n",
    "    cols = set([v for v in df_copy.columns.values if \"micro_\" in v])\n",
    "    for c in cols:\n",
    "        df_copy[c] = df[c].apply(lambda d: fmt_str.format(d))  \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Per Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_algo_name(coll):\n",
    "    if \"WINDOW\" in coll:\n",
    "        return \"Window-Based Tagger\"\n",
    "    if \"CRF\" in coll:\n",
    "        return \"CRF\"\n",
    "    if \"HMM\" in coll:\n",
    "        return \"HMM\"\n",
    "    if \"PERCEPTRON\" in coll:\n",
    "        return \"Structured Perceptron\"\n",
    "    if \"RNN\" in coll:\n",
    "        return \"Bidirectional RNN\"\n",
    "    \n",
    "def get_dataset_name(coll):\n",
    "    if \"CB\" in coll:\n",
    "        return \"Coral Bleaching\"\n",
    "    if \"SC\" in coll:\n",
    "        return \"Skin Cancer\"\n",
    "    raise Exception(\"Unable to determined dataset name in method - get_dataset_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(code='50', f1=0.99, prec=0.98, rec=0.97)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Metric = namedtuple(\"Metric\", \"code f1 prec rec\")\n",
    "Metric(code=\"50\", f1=0.99, prec=0.98, rec=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MICRO_F1 = \"MICRO_F1\"\n",
    "MACRO_F1 = \"MACRO_F1\"\n",
    "\n",
    "def sort_key_by_code(code):\n",
    "    if \"b\" in code:\n",
    "        code = code.replace(\"b\", \".1\")\n",
    "    if code == MICRO_F1:\n",
    "        return 9999999\n",
    "    return float(code)\n",
    "\n",
    "def sort_key(metric):\n",
    "    code = metric.code\n",
    "    return sort_key_by_code(metric.code)\n",
    "\n",
    "def get_metrics_by_code(coll):\n",
    "    metrics_by_code = {}\n",
    "    count = 0\n",
    "    for row in db[coll].find({}):\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            raise Exception(\"Multiple rows found in get_metrics_by_code for collection: \" + coll)\n",
    "        for k in row.keys():\n",
    "            if k[0].isdigit():\n",
    "                code, f1, prec, rec = k, row[k][\"f1_score\"], row[k][\"precision\"], row[k][\"recall\"]\n",
    "                metric = Metric(code=code, f1=f1, prec=prec, rec=rec)\n",
    "                metrics_by_code[code] = metric\n",
    "    return metrics_by_code\n",
    "\n",
    "def compute_metrics_by_class(coll):\n",
    "    print(\"code\\t\\tf1\\trec\\tprec\")\n",
    "    count = 0\n",
    "    for row in db[coll].find({}):\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            raise Exception(\"More than one row found for collection: %s method - compute_metrics_by_class\" % (coll))        \n",
    "        keys = []                \n",
    "        metrics = []\n",
    "        f1s, recs, precs = [],[],[]\n",
    "        macro_f1 = -1\n",
    "        for k in row.keys():\n",
    "            if k[0].isdigit() or k == MICRO_F1:\n",
    "                keys.append(k)\n",
    "                code, f1, prec, rec = k, row[k][\"f1_score\"], row[k][\"precision\"], row[k][\"recall\"]\n",
    "                metric = Metric(code=code, f1=f1, prec=prec, rec=rec)\n",
    "                metrics.append(metric)                \n",
    "                # Only append metrics if a code (to ensure macro f1 is computed correctly)\n",
    "                if k[0].isdigit():\n",
    "                    f1s.append(f1)\n",
    "                    recs.append(rec)\n",
    "                    precs.append(prec)\n",
    "                \n",
    "            if k == MACRO_F1:\n",
    "                macro_f1 = row[k]\n",
    "        \n",
    "        for metric in sorted(metrics, key = sort_key):\n",
    "            code = metric.code\n",
    "            if code == MICRO_F1:\n",
    "                print()\n",
    "                code = \"MICRO\"\n",
    "            print(\"{code}\\t{f1:.4f}\\t{rec:.4f}\\t{prec:.4f}\".format(code=code.ljust(10), f1=metric.f1, prec=metric.prec, rec=metric.rec))\n",
    "\n",
    "        macro_rec  = np.mean(recs)\n",
    "        macro_prec = np.mean(precs)\n",
    "        macro_f1_calc = (2 * (macro_rec * macro_prec)) / (macro_rec + macro_prec)\n",
    "        \n",
    "        print(\"{code}\\t{f1:.4f}\\t{rec:.4f}\\t{prec:.4f}\".format(\n",
    "            code=\"MACRO\".ljust(10), \n",
    "            f1=macro_f1, \n",
    "            prec=macro_rec, \n",
    "            rec=macro_prec))\n",
    "\n",
    "        # Spot Check Macro F1\n",
    "        #print(\"{code}\\t{f1:.4f}\\t{rec:.4f}\\t{prec:.4f}\".format(\n",
    "        #    code=\"MACRO_Calc\".ljust(10), \n",
    "        #   f1=macro_f1_calc, \n",
    "        #    prec=macro_rec, \n",
    "        #    rec=macro_prec))\n",
    "        \n",
    "        print(\"\\n\" + \",\".join(sorted(keys)))\n",
    "        print(\"\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window-Based Tagger\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8260\t0.7960\t0.8584\n",
      "2         \t0.7399\t0.6871\t0.8016\n",
      "3         \t0.8274\t0.7814\t0.8792\n",
      "4         \t0.8318\t0.8193\t0.8447\n",
      "5         \t0.4490\t0.3113\t0.8049\n",
      "5b        \t0.0303\t0.0227\t0.0455\n",
      "6         \t0.8357\t0.8018\t0.8725\n",
      "7         \t0.8381\t0.7598\t0.9344\n",
      "11        \t0.8995\t0.8775\t0.9227\n",
      "12        \t0.8629\t0.7798\t0.9659\n",
      "13        \t0.7340\t0.6921\t0.7814\n",
      "14        \t0.7484\t0.7437\t0.7532\n",
      "50        \t0.9041\t0.8803\t0.9293\n",
      "\n",
      "MICRO     \t0.8415\t0.8023\t0.8849\n",
      "MACRO     \t0.7400\t0.7995\t0.6887\n",
      "\n",
      "1,11,12,13,14,2,3,4,5,50,5b,6,7,MICRO_F1\n",
      "\n",
      "\n",
      "CRF\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8365\t0.8007\t0.8756\n",
      "2         \t0.7622\t0.7415\t0.7842\n",
      "3         \t0.8197\t0.7737\t0.8715\n",
      "4         \t0.7934\t0.7922\t0.7946\n",
      "5         \t0.3188\t0.2075\t0.6875\n",
      "5b        \t0.0000\t0.0000\t0.0000\n",
      "6         \t0.8337\t0.7793\t0.8964\n",
      "7         \t0.8263\t0.7394\t0.9364\n",
      "11        \t0.9268\t0.9314\t0.9223\n",
      "12        \t0.8629\t0.7798\t0.9659\n",
      "13        \t0.7291\t0.6794\t0.7868\n",
      "14        \t0.7002\t0.6835\t0.7176\n",
      "50        \t0.9000\t0.8839\t0.9167\n",
      "\n",
      "MICRO     \t0.8355\t0.7967\t0.8783\n",
      "MACRO     \t0.7250\t0.7812\t0.6763\n",
      "\n",
      "1,11,12,13,14,2,3,4,5,50,5b,6,7,MICRO_F1\n",
      "\n",
      "\n",
      "HMM\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.7838\t0.8523\t0.7255\n",
      "2         \t0.4413\t0.8435\t0.2988\n",
      "3         \t0.7330\t0.7277\t0.7383\n",
      "4         \t0.7075\t0.7651\t0.6580\n",
      "5         \t0.1525\t0.2453\t0.1106\n",
      "5b        \t0.0730\t0.2955\t0.0417\n",
      "6         \t0.8208\t0.7838\t0.8614\n",
      "7         \t0.7311\t0.6531\t0.8303\n",
      "11        \t0.8692\t0.9118\t0.8304\n",
      "12        \t0.6984\t0.8073\t0.6154\n",
      "13        \t0.7022\t0.7937\t0.6297\n",
      "14        \t0.7148\t0.8766\t0.6035\n",
      "50        \t0.8713\t0.8644\t0.8784\n",
      "\n",
      "MICRO     \t0.7471\t0.7986\t0.7018\n",
      "MACRO     \t0.6575\t0.6017\t0.7246\n",
      "\n",
      "1,11,12,13,14,2,3,4,5,50,5b,6,7,MICRO_F1\n",
      "\n",
      "\n",
      "Structured Perceptron\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8391\t0.8195\t0.8598\n",
      "2         \t0.7826\t0.7347\t0.8372\n",
      "3         \t0.8085\t0.7507\t0.8758\n",
      "4         \t0.8265\t0.8253\t0.8278\n",
      "5         \t0.4459\t0.3302\t0.6863\n",
      "5b        \t0.0313\t0.0227\t0.0500\n",
      "6         \t0.8357\t0.8018\t0.8725\n",
      "7         \t0.8199\t0.7253\t0.9429\n",
      "11        \t0.8933\t0.8824\t0.9045\n",
      "12        \t0.8824\t0.8257\t0.9474\n",
      "13        \t0.7165\t0.6540\t0.7923\n",
      "14        \t0.7307\t0.7342\t0.7273\n",
      "50        \t0.9015\t0.8709\t0.9343\n",
      "\n",
      "MICRO     \t0.8367\t0.7942\t0.8840\n",
      "MACRO     \t0.7365\t0.7891\t0.6906\n",
      "\n",
      "1,11,12,13,14,2,3,4,5,50,5b,6,7,MICRO_F1\n",
      "\n",
      "\n",
      "Bidirectional RNN\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8185\t0.8593\t0.7814\n",
      "2         \t0.7122\t0.6735\t0.7557\n",
      "3         \t0.8273\t0.7737\t0.8888\n",
      "4         \t0.8453\t0.8886\t0.8060\n",
      "5         \t0.5868\t0.6698\t0.5221\n",
      "5b        \t0.3077\t0.2727\t0.3529\n",
      "6         \t0.8325\t0.7838\t0.8878\n",
      "7         \t0.8397\t0.7645\t0.9312\n",
      "11        \t0.9086\t0.9020\t0.9154\n",
      "12        \t0.9390\t0.9174\t0.9615\n",
      "13        \t0.7528\t0.7397\t0.7664\n",
      "14        \t0.6870\t0.6772\t0.6971\n",
      "50        \t0.9078\t0.9034\t0.9123\n",
      "\n",
      "MICRO     \t0.8423\t0.8297\t0.8553\n",
      "MACRO     \t0.7692\t0.7830\t0.7558\n",
      "\n",
      "1,11,12,13,14,2,3,4,5,50,5b,6,7,MICRO_F1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "\n",
    "    #collection = collection.replace(\"CB\",\"SC\")\n",
    "    print(get_algo_name(collection))\n",
    "    compute_metrics_by_class(collection)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window-Based Tagger\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8262\t0.7888\t0.8673\n",
      "2         \t0.8525\t0.8445\t0.8606\n",
      "3         \t0.8349\t0.8185\t0.8520\n",
      "4         \t0.7330\t0.6786\t0.7969\n",
      "5         \t0.8518\t0.8341\t0.8702\n",
      "6         \t0.6788\t0.5525\t0.8799\n",
      "11        \t0.6214\t0.4848\t0.8649\n",
      "12        \t0.5294\t0.4091\t0.7500\n",
      "50        \t0.8355\t0.8246\t0.8467\n",
      "\n",
      "1,11,12,2,3,4,5,50,6\n",
      "\n",
      "\n",
      "CRF\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8011\t0.7419\t0.8706\n",
      "2         \t0.8347\t0.8434\t0.8262\n",
      "3         \t0.7916\t0.7700\t0.8144\n",
      "4         \t0.7312\t0.6673\t0.8087\n",
      "5         \t0.8509\t0.8249\t0.8785\n",
      "6         \t0.7061\t0.5869\t0.8861\n",
      "11        \t0.6604\t0.5303\t0.8750\n",
      "12        \t0.5294\t0.4091\t0.7500\n",
      "50        \t0.8308\t0.7924\t0.8731\n",
      "\n",
      "1,11,12,2,3,4,5,50,6\n",
      "\n",
      "\n",
      "HMM\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.7121\t0.7992\t0.6421\n",
      "2         \t0.7450\t0.7777\t0.7150\n",
      "3         \t0.7247\t0.8442\t0.6348\n",
      "4         \t0.5288\t0.5865\t0.4815\n",
      "5         \t0.8018\t0.8157\t0.7884\n",
      "6         \t0.5796\t0.6984\t0.4953\n",
      "11        \t0.6066\t0.5606\t0.6607\n",
      "12        \t0.4027\t0.3409\t0.4918\n",
      "50        \t0.6413\t0.6768\t0.6094\n",
      "\n",
      "1,11,12,2,3,4,5,50,6\n",
      "\n",
      "\n",
      "Structured Perceptron\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8132\t0.7586\t0.8762\n",
      "2         \t0.8464\t0.8372\t0.8559\n",
      "3         \t0.8276\t0.8064\t0.8501\n",
      "4         \t0.7295\t0.6617\t0.8129\n",
      "5         \t0.8545\t0.8456\t0.8635\n",
      "6         \t0.6734\t0.5443\t0.8830\n",
      "11        \t0.6038\t0.4848\t0.8000\n",
      "12        \t0.5571\t0.4432\t0.7500\n",
      "50        \t0.8450\t0.8251\t0.8659\n",
      "\n",
      "1,11,12,2,3,4,5,50,6\n",
      "\n",
      "\n",
      "Bidirectional RNN\n",
      "code\t\tf1\trec\tprec\n",
      "1         \t0.8530\t0.8210\t0.8875\n",
      "2         \t0.8534\t0.8570\t0.8499\n",
      "3         \t0.8360\t0.7867\t0.8919\n",
      "4         \t0.7500\t0.6880\t0.8243\n",
      "5         \t0.8617\t0.8468\t0.8771\n",
      "6         \t0.7507\t0.6836\t0.8323\n",
      "11        \t0.6667\t0.5152\t0.9444\n",
      "12        \t0.4724\t0.3409\t0.7692\n",
      "50        \t0.8703\t0.8610\t0.8799\n",
      "\n",
      "1,11,12,2,3,4,5,50,6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "    collection = collection.replace(\"CB\",\"SC\")\n",
    "    print(get_algo_name(collection))\n",
    "    compute_metrics_by_class(collection)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Latex Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"\"\"\n",
    "\\\\begin{table}[H]\n",
    "\\\\small\n",
    "\\\\caption{Test Data Metrics for the ALGO on the DATASET Dataset}\n",
    "\\\\center{\\\\begin{tabular}{l c c c}\n",
    "\\\\toprule\n",
    "\\\\textbf{Code} & \\\\textbf{$F_1$} & \\\\textbf{Recall} & \\\\textbf{Precision}\\\\\\\\\n",
    "\\\\midrule\"\"\"\n",
    "# Have to print {{ and }} to escape the curly braces in format string\n",
    "# Add in the format strings later\n",
    "header = header \\\n",
    ".replace(\"{\",\"{{\").replace(\"}\",\"}}\") \\\n",
    ".replace(\"ALGO\",\"{algo}\") \\\n",
    ".replace(\"DATASET\",\"{dataset}\")\n",
    "\n",
    "footer = \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}}\n",
    "\\\\label{table:rq1_ALGO_DATASET}\n",
    "\\\\end{table}\"\"\"\n",
    "# Have to print {{ and }} to escape the curly braces in format string\n",
    "footer = footer \\\n",
    ".replace(\"{\",\"{{\").replace(\"}\",\"}}\") \\\n",
    ".replace(\"ALGO\", \"{algo_for_table_lbl_metrics}\") \\\n",
    ".replace(\"DATASET\", \"{dataset_for_table_lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics_by_class_LATEX(coll):\n",
    "    algo_name = get_algo_name(coll)\n",
    "    dataset = get_dataset_name(coll)\n",
    "    \n",
    "    print(header.format(algo=algo_name, dataset=dataset))\n",
    "    count = 0\n",
    "    for row in db[coll].find({}):\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            raise Exception(\"More than one row found for collection: %s method - compute_metrics_by_class\" % (coll))        \n",
    "        keys = []                \n",
    "        metrics = []\n",
    "        f1s, recs, precs = [],[],[]\n",
    "        macro_f1 = -1\n",
    "        for k in row.keys():\n",
    "            if k[0].isdigit() or k == MICRO_F1:\n",
    "                keys.append(k)\n",
    "                code, f1, prec, rec = k, row[k][\"f1_score\"], row[k][\"precision\"], row[k][\"recall\"]\n",
    "                metric = Metric(code=code, f1=f1, prec=prec, rec=rec)\n",
    "                metrics.append(metric)                \n",
    "                # Only append metrics if a code (to ensure macro f1 is computed correctly)\n",
    "                if k[0].isdigit():\n",
    "                    f1s.append(f1)\n",
    "                    recs.append(rec)\n",
    "                    precs.append(prec)\n",
    "                \n",
    "            if k == MACRO_F1:\n",
    "                macro_f1 = row[k]\n",
    "        \n",
    "        row_template = \"\"\"{code} &\t{f1:.3f}\t\t\t&\t{rec:.3f}\t\t\t&\t{prec:.3f} \\\\\\\\ \"\"\"\n",
    "        for metric in sorted(metrics, key = sort_key):\n",
    "            code = metric.code\n",
    "            if code == MICRO_F1:\n",
    "                print(\"\\midrule\")\n",
    "                code = \"Micro\"\n",
    "            print(row_template.format(code=code, f1=metric.f1, prec=metric.prec, rec=metric.rec))\n",
    "\n",
    "        macro_rec  = np.mean(recs)\n",
    "        macro_prec = np.mean(precs)\n",
    "        macro_f1_calc = (2 * (macro_rec * macro_prec)) / (macro_rec + macro_prec)       \n",
    "        \n",
    "        print(row_template.format(code=\"Macro\", f1=macro_f1_calc, prec=macro_prec, rec=macro_rec)) \n",
    "        \n",
    "        # Footer\n",
    "        #   \\label{{table:rq1_{{algo_for_table_lbl_metrics}}_{{dataset_for_table_lbl}}}}\n",
    "        dataset_abbrev = \"\".join([word[0] for word in dataset.lower().split(\" \")])\n",
    "        print(footer.format(\n",
    "                algo_for_table_lbl_metrics=algo_name.lower().strip().replace(\" \",\"_\").replace(\"-\",\"_\"),\n",
    "                dataset_for_table_lbl=dataset_abbrev\n",
    "        ))\n",
    "    \n",
    "        #print(\"*\" * 60)\n",
    "        #print(\"\\n\" + \",\".join(sorted(keys)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\section{Coral Bleaching}\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the Window-Based Tagger on the Coral Bleaching Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.826\t\t\t&\t0.796\t\t\t&\t0.858 \\\\ \n",
      "2 &\t0.740\t\t\t&\t0.687\t\t\t&\t0.802 \\\\ \n",
      "3 &\t0.827\t\t\t&\t0.781\t\t\t&\t0.879 \\\\ \n",
      "4 &\t0.832\t\t\t&\t0.819\t\t\t&\t0.845 \\\\ \n",
      "5 &\t0.449\t\t\t&\t0.311\t\t\t&\t0.805 \\\\ \n",
      "5b &\t0.030\t\t\t&\t0.023\t\t\t&\t0.045 \\\\ \n",
      "6 &\t0.836\t\t\t&\t0.802\t\t\t&\t0.873 \\\\ \n",
      "7 &\t0.838\t\t\t&\t0.760\t\t\t&\t0.934 \\\\ \n",
      "11 &\t0.899\t\t\t&\t0.877\t\t\t&\t0.923 \\\\ \n",
      "12 &\t0.863\t\t\t&\t0.780\t\t\t&\t0.966 \\\\ \n",
      "13 &\t0.734\t\t\t&\t0.692\t\t\t&\t0.781 \\\\ \n",
      "14 &\t0.748\t\t\t&\t0.744\t\t\t&\t0.753 \\\\ \n",
      "50 &\t0.904\t\t\t&\t0.880\t\t\t&\t0.929 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.842\t\t\t&\t0.802\t\t\t&\t0.885 \\\\ \n",
      "Macro &\t0.740\t\t\t&\t0.689\t\t\t&\t0.800 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_window_based_tagger_cb}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the CRF on the Coral Bleaching Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.836\t\t\t&\t0.801\t\t\t&\t0.876 \\\\ \n",
      "2 &\t0.762\t\t\t&\t0.741\t\t\t&\t0.784 \\\\ \n",
      "3 &\t0.820\t\t\t&\t0.774\t\t\t&\t0.871 \\\\ \n",
      "4 &\t0.793\t\t\t&\t0.792\t\t\t&\t0.795 \\\\ \n",
      "5 &\t0.319\t\t\t&\t0.208\t\t\t&\t0.688 \\\\ \n",
      "5b &\t0.000\t\t\t&\t0.000\t\t\t&\t0.000 \\\\ \n",
      "6 &\t0.834\t\t\t&\t0.779\t\t\t&\t0.896 \\\\ \n",
      "7 &\t0.826\t\t\t&\t0.739\t\t\t&\t0.936 \\\\ \n",
      "11 &\t0.927\t\t\t&\t0.931\t\t\t&\t0.922 \\\\ \n",
      "12 &\t0.863\t\t\t&\t0.780\t\t\t&\t0.966 \\\\ \n",
      "13 &\t0.729\t\t\t&\t0.679\t\t\t&\t0.787 \\\\ \n",
      "14 &\t0.700\t\t\t&\t0.684\t\t\t&\t0.718 \\\\ \n",
      "50 &\t0.900\t\t\t&\t0.884\t\t\t&\t0.917 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.835\t\t\t&\t0.797\t\t\t&\t0.878 \\\\ \n",
      "Macro &\t0.725\t\t\t&\t0.676\t\t\t&\t0.781 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_crf_cb}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the HMM on the Coral Bleaching Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.784\t\t\t&\t0.852\t\t\t&\t0.726 \\\\ \n",
      "2 &\t0.441\t\t\t&\t0.844\t\t\t&\t0.299 \\\\ \n",
      "3 &\t0.733\t\t\t&\t0.728\t\t\t&\t0.738 \\\\ \n",
      "4 &\t0.708\t\t\t&\t0.765\t\t\t&\t0.658 \\\\ \n",
      "5 &\t0.152\t\t\t&\t0.245\t\t\t&\t0.111 \\\\ \n",
      "5b &\t0.073\t\t\t&\t0.295\t\t\t&\t0.042 \\\\ \n",
      "6 &\t0.821\t\t\t&\t0.784\t\t\t&\t0.861 \\\\ \n",
      "7 &\t0.731\t\t\t&\t0.653\t\t\t&\t0.830 \\\\ \n",
      "11 &\t0.869\t\t\t&\t0.912\t\t\t&\t0.830 \\\\ \n",
      "12 &\t0.698\t\t\t&\t0.807\t\t\t&\t0.615 \\\\ \n",
      "13 &\t0.702\t\t\t&\t0.794\t\t\t&\t0.630 \\\\ \n",
      "14 &\t0.715\t\t\t&\t0.877\t\t\t&\t0.603 \\\\ \n",
      "50 &\t0.871\t\t\t&\t0.864\t\t\t&\t0.878 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.747\t\t\t&\t0.799\t\t\t&\t0.702 \\\\ \n",
      "Macro &\t0.657\t\t\t&\t0.725\t\t\t&\t0.602 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_hmm_cb}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the Structured Perceptron on the Coral Bleaching Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.839\t\t\t&\t0.819\t\t\t&\t0.860 \\\\ \n",
      "2 &\t0.783\t\t\t&\t0.735\t\t\t&\t0.837 \\\\ \n",
      "3 &\t0.808\t\t\t&\t0.751\t\t\t&\t0.876 \\\\ \n",
      "4 &\t0.827\t\t\t&\t0.825\t\t\t&\t0.828 \\\\ \n",
      "5 &\t0.446\t\t\t&\t0.330\t\t\t&\t0.686 \\\\ \n",
      "5b &\t0.031\t\t\t&\t0.023\t\t\t&\t0.050 \\\\ \n",
      "6 &\t0.836\t\t\t&\t0.802\t\t\t&\t0.873 \\\\ \n",
      "7 &\t0.820\t\t\t&\t0.725\t\t\t&\t0.943 \\\\ \n",
      "11 &\t0.893\t\t\t&\t0.882\t\t\t&\t0.905 \\\\ \n",
      "12 &\t0.882\t\t\t&\t0.826\t\t\t&\t0.947 \\\\ \n",
      "13 &\t0.717\t\t\t&\t0.654\t\t\t&\t0.792 \\\\ \n",
      "14 &\t0.731\t\t\t&\t0.734\t\t\t&\t0.727 \\\\ \n",
      "50 &\t0.901\t\t\t&\t0.871\t\t\t&\t0.934 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.837\t\t\t&\t0.794\t\t\t&\t0.884 \\\\ \n",
      "Macro &\t0.737\t\t\t&\t0.691\t\t\t&\t0.789 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_structured_perceptron_cb}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the Bidirectional RNN on the Coral Bleaching Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.819\t\t\t&\t0.859\t\t\t&\t0.781 \\\\ \n",
      "2 &\t0.712\t\t\t&\t0.673\t\t\t&\t0.756 \\\\ \n",
      "3 &\t0.827\t\t\t&\t0.774\t\t\t&\t0.889 \\\\ \n",
      "4 &\t0.845\t\t\t&\t0.889\t\t\t&\t0.806 \\\\ \n",
      "5 &\t0.587\t\t\t&\t0.670\t\t\t&\t0.522 \\\\ \n",
      "5b &\t0.308\t\t\t&\t0.273\t\t\t&\t0.353 \\\\ \n",
      "6 &\t0.833\t\t\t&\t0.784\t\t\t&\t0.888 \\\\ \n",
      "7 &\t0.840\t\t\t&\t0.765\t\t\t&\t0.931 \\\\ \n",
      "11 &\t0.909\t\t\t&\t0.902\t\t\t&\t0.915 \\\\ \n",
      "12 &\t0.939\t\t\t&\t0.917\t\t\t&\t0.962 \\\\ \n",
      "13 &\t0.753\t\t\t&\t0.740\t\t\t&\t0.766 \\\\ \n",
      "14 &\t0.687\t\t\t&\t0.677\t\t\t&\t0.697 \\\\ \n",
      "50 &\t0.908\t\t\t&\t0.903\t\t\t&\t0.912 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.842\t\t\t&\t0.830\t\t\t&\t0.855 \\\\ \n",
      "Macro &\t0.769\t\t\t&\t0.756\t\t\t&\t0.783 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_bidirectional_rnn_cb}\n",
      "\\end{table}\n",
      "\n",
      "\\section{Skin Cancer}\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the Window-Based Tagger on the Skin Cancer Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.826\t\t\t&\t0.789\t\t\t&\t0.867 \\\\ \n",
      "2 &\t0.852\t\t\t&\t0.844\t\t\t&\t0.861 \\\\ \n",
      "3 &\t0.835\t\t\t&\t0.818\t\t\t&\t0.852 \\\\ \n",
      "4 &\t0.733\t\t\t&\t0.679\t\t\t&\t0.797 \\\\ \n",
      "5 &\t0.852\t\t\t&\t0.834\t\t\t&\t0.870 \\\\ \n",
      "6 &\t0.679\t\t\t&\t0.552\t\t\t&\t0.880 \\\\ \n",
      "11 &\t0.621\t\t\t&\t0.485\t\t\t&\t0.865 \\\\ \n",
      "12 &\t0.529\t\t\t&\t0.409\t\t\t&\t0.750 \\\\ \n",
      "50 &\t0.836\t\t\t&\t0.825\t\t\t&\t0.847 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.814\t\t\t&\t0.779\t\t\t&\t0.853 \\\\ \n",
      "Macro &\t0.761\t\t\t&\t0.693\t\t\t&\t0.843 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_window_based_tagger_sc}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the CRF on the Skin Cancer Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.801\t\t\t&\t0.742\t\t\t&\t0.871 \\\\ \n",
      "2 &\t0.835\t\t\t&\t0.843\t\t\t&\t0.826 \\\\ \n",
      "3 &\t0.792\t\t\t&\t0.770\t\t\t&\t0.814 \\\\ \n",
      "4 &\t0.731\t\t\t&\t0.667\t\t\t&\t0.809 \\\\ \n",
      "5 &\t0.851\t\t\t&\t0.825\t\t\t&\t0.879 \\\\ \n",
      "6 &\t0.706\t\t\t&\t0.587\t\t\t&\t0.886 \\\\ \n",
      "11 &\t0.660\t\t\t&\t0.530\t\t\t&\t0.875 \\\\ \n",
      "12 &\t0.529\t\t\t&\t0.409\t\t\t&\t0.750 \\\\ \n",
      "50 &\t0.831\t\t\t&\t0.792\t\t\t&\t0.873 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.804\t\t\t&\t0.759\t\t\t&\t0.855 \\\\ \n",
      "Macro &\t0.756\t\t\t&\t0.685\t\t\t&\t0.843 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_crf_sc}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the HMM on the Skin Cancer Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.712\t\t\t&\t0.799\t\t\t&\t0.642 \\\\ \n",
      "2 &\t0.745\t\t\t&\t0.778\t\t\t&\t0.715 \\\\ \n",
      "3 &\t0.725\t\t\t&\t0.844\t\t\t&\t0.635 \\\\ \n",
      "4 &\t0.529\t\t\t&\t0.586\t\t\t&\t0.481 \\\\ \n",
      "5 &\t0.802\t\t\t&\t0.816\t\t\t&\t0.788 \\\\ \n",
      "6 &\t0.580\t\t\t&\t0.698\t\t\t&\t0.495 \\\\ \n",
      "11 &\t0.607\t\t\t&\t0.561\t\t\t&\t0.661 \\\\ \n",
      "12 &\t0.403\t\t\t&\t0.341\t\t\t&\t0.492 \\\\ \n",
      "50 &\t0.641\t\t\t&\t0.677\t\t\t&\t0.609 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.675\t\t\t&\t0.731\t\t\t&\t0.628 \\\\ \n",
      "Macro &\t0.644\t\t\t&\t0.678\t\t\t&\t0.613 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_hmm_sc}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the Structured Perceptron on the Skin Cancer Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.813\t\t\t&\t0.759\t\t\t&\t0.876 \\\\ \n",
      "2 &\t0.846\t\t\t&\t0.837\t\t\t&\t0.856 \\\\ \n",
      "3 &\t0.828\t\t\t&\t0.806\t\t\t&\t0.850 \\\\ \n",
      "4 &\t0.730\t\t\t&\t0.662\t\t\t&\t0.813 \\\\ \n",
      "5 &\t0.854\t\t\t&\t0.846\t\t\t&\t0.864 \\\\ \n",
      "6 &\t0.673\t\t\t&\t0.544\t\t\t&\t0.883 \\\\ \n",
      "11 &\t0.604\t\t\t&\t0.485\t\t\t&\t0.800 \\\\ \n",
      "12 &\t0.557\t\t\t&\t0.443\t\t\t&\t0.750 \\\\ \n",
      "50 &\t0.845\t\t\t&\t0.825\t\t\t&\t0.866 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.814\t\t\t&\t0.773\t\t\t&\t0.860 \\\\ \n",
      "Macro &\t0.757\t\t\t&\t0.690\t\t\t&\t0.840 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_structured_perceptron_sc}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\\small\n",
      "\\caption{Test Data Metrics for the Bidirectional RNN on the Skin Cancer Dataset}\n",
      "\\center{\\begin{tabular}{l c c c}\n",
      "\\toprule\n",
      "\\textbf{Code} & \\textbf{$F_1$} & \\textbf{Recall} & \\textbf{Precision}\\\\\n",
      "\\midrule\n",
      "1 &\t0.853\t\t\t&\t0.821\t\t\t&\t0.888 \\\\ \n",
      "2 &\t0.853\t\t\t&\t0.857\t\t\t&\t0.850 \\\\ \n",
      "3 &\t0.836\t\t\t&\t0.787\t\t\t&\t0.892 \\\\ \n",
      "4 &\t0.750\t\t\t&\t0.688\t\t\t&\t0.824 \\\\ \n",
      "5 &\t0.862\t\t\t&\t0.847\t\t\t&\t0.877 \\\\ \n",
      "6 &\t0.751\t\t\t&\t0.684\t\t\t&\t0.832 \\\\ \n",
      "11 &\t0.667\t\t\t&\t0.515\t\t\t&\t0.944 \\\\ \n",
      "12 &\t0.472\t\t\t&\t0.341\t\t\t&\t0.769 \\\\ \n",
      "50 &\t0.870\t\t\t&\t0.861\t\t\t&\t0.880 \\\\ \n",
      "\\midrule\n",
      "Micro &\t0.837\t\t\t&\t0.807\t\t\t&\t0.869 \\\\ \n",
      "Macro &\t0.779\t\t\t&\t0.711\t\t\t&\t0.862 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}}\n",
      "\\label{table:rq1_bidirectional_rnn_sc}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CB\n",
    "print(\"\\\\section{Coral Bleaching}\")\n",
    "for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "    compute_metrics_by_class_LATEX(collection)\n",
    "    print(\"\")\n",
    "\n",
    "#SC \n",
    "print(\"\\\\section{Skin Cancer}\")\n",
    "for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "    collection = collection.replace(\"CB\",\"SC\")\n",
    "    compute_metrics_by_class_LATEX(collection)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Best Algorithm Per Class, By Metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_codes = \"1,2,3,4,5,5b,6,7,11,12,13,14,50\".split(\",\")\n",
    "sc_codes = \"1,2,3,4,5,6,11,12,50\".split(\",\")\n",
    "len(cb_codes), len(sc_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric4Code = namedtuple(\"Metric4Code\", \"algo f1 rec prec\")\n",
    "\n",
    "def sort_by_f1_desc(metric):\n",
    "    return -metric.f1\n",
    "\n",
    "def sort_by_rec_desc(metric):\n",
    "    return -metric.rec\n",
    "\n",
    "def sort_by_prec_desc(metric):\n",
    "    return -metric.prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: 1  \tF1:Structured Perceptron \tRec:Bidirectional RNN     \tPrec:CRF                   \n",
      "Code: 2  \tF1:Structured Perceptron \tRec:HMM                   \tPrec:Structured Perceptron \n",
      "Code: 3  \tF1:Window-Based Tagger   \tRec:Window-Based Tagger   \tPrec:Bidirectional RNN     \n",
      "Code: 4  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Window-Based Tagger   \n",
      "Code: 5  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Window-Based Tagger   \n",
      "Code: 5b \tF1:Bidirectional RNN     \tRec:HMM                   \tPrec:Bidirectional RNN     \n",
      "Code: 6  \tF1:Window-Based Tagger   \tRec:Window-Based Tagger   \tPrec:CRF                   \n",
      "Code: 7  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Structured Perceptron \n",
      "Code: 11 \tF1:CRF                   \tRec:CRF                   \tPrec:Window-Based Tagger   \n",
      "Code: 12 \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Window-Based Tagger   \n",
      "Code: 13 \tF1:Bidirectional RNN     \tRec:HMM                   \tPrec:Structured Perceptron \n",
      "Code: 14 \tF1:Window-Based Tagger   \tRec:HMM                   \tPrec:Window-Based Tagger   \n",
      "Code: 50 \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Structured Perceptron \n"
     ]
    }
   ],
   "source": [
    "ALGO_NAME_WIDTH = len(\"Structured Perceptron\") + 1\n",
    "\n",
    "for code in cb_codes:\n",
    "    metrics_for_code = []\n",
    "    for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "        m4code = get_metrics_by_code(collection)[code]   \n",
    "        algo_name = get_algo_name(collection)\n",
    "        metrics_for_code.append(Metric4Code(algo=algo_name, f1=m4code.f1, rec=m4code.rec, prec=m4code.prec))\n",
    "    \n",
    "    best_f1 = sorted(metrics_for_code, key=sort_by_f1_desc)[0]\n",
    "    best_rec = sorted(metrics_for_code, key=sort_by_rec_desc)[0]\n",
    "    best_prec = sorted(metrics_for_code, key=sort_by_prec_desc)[0]\n",
    "    print(\"Code: {code}\\tF1:{f1}\\tRec:{rec}\\tPrec:{prec}\".format(\n",
    "        code=code.ljust(3), \n",
    "        f1=best_f1.algo.ljust(ALGO_NAME_WIDTH), \n",
    "        rec=best_rec.algo.ljust(ALGO_NAME_WIDTH), \n",
    "        prec=best_prec.algo.ljust(ALGO_NAME_WIDTH)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for code in cb_codes:\n",
    "    metrics_for_code = []\n",
    "    for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "        #print(collection)\n",
    "        m4code = get_metrics_by_code(collection)[code]   \n",
    "        algo_name = get_algo_name(collection)\n",
    "        metrics_for_code.append(Metric4Code(algo=algo_name, f1=m4code.f1, rec=m4code.rec, prec=m4code.prec))\n",
    "    \n",
    "    best_f1 = sorted(metrics_for_code, key=sort_by_f1_desc)[0]\n",
    "    best_rec = sorted(metrics_for_code, key=sort_by_rec_desc)[0]\n",
    "    best_prec = sorted(metrics_for_code, key=sort_by_prec_desc)[0]\n",
    "    print(\"Code: {code}\\tF1:{f1}\\tRec:{rec}\\tPrec:{prec}\".format(\n",
    "        code=code.ljust(3), \n",
    "        f1=best_f1.algo.ljust(ALGO_NAME_WIDTH), \n",
    "        rec=best_rec.algo.ljust(ALGO_NAME_WIDTH), \n",
    "        prec=best_prec.algo.ljust(ALGO_NAME_WIDTH)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: 1  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Bidirectional RNN     \n",
      "Code: 2  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Window-Based Tagger   \n",
      "Code: 3  \tF1:Bidirectional RNN     \tRec:HMM                   \tPrec:Bidirectional RNN     \n",
      "Code: 4  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Bidirectional RNN     \n",
      "Code: 5  \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:CRF                   \n",
      "Code: 6  \tF1:Bidirectional RNN     \tRec:HMM                   \tPrec:CRF                   \n",
      "Code: 11 \tF1:Bidirectional RNN     \tRec:HMM                   \tPrec:Bidirectional RNN     \n",
      "Code: 12 \tF1:Structured Perceptron \tRec:Structured Perceptron \tPrec:Bidirectional RNN     \n",
      "Code: 50 \tF1:Bidirectional RNN     \tRec:Bidirectional RNN     \tPrec:Bidirectional RNN     \n"
     ]
    }
   ],
   "source": [
    "for code in sc_codes:\n",
    "    metrics_for_code = []\n",
    "    for collection in \"TEST_CB_TAGGING_VD_WINDOW_CLASSIFIER_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_CRF_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_HMM_MOST_COMMON_TAG_MULTICLASS,TEST_CB_TAGGING_VD_AVG_PERCEPTRON_MOST_COMMON_TAG,TEST_CB_TAGGING_VD_RNN_MOST_COMMON_TAG\".split(\",\"):\n",
    "        collection = collection.replace(\"CB_\", \"SC_\")\n",
    "        m4code = get_metrics_by_code(collection)[code]   \n",
    "        algo_name = get_algo_name(collection)\n",
    "        metrics_for_code.append(Metric4Code(algo=algo_name, f1=m4code.f1, rec=m4code.rec, prec=m4code.prec))\n",
    "    \n",
    "    best_f1 = sorted(metrics_for_code, key=sort_by_f1_desc)[0]\n",
    "    best_rec = sorted(metrics_for_code, key=sort_by_rec_desc)[0]\n",
    "    best_prec = sorted(metrics_for_code, key=sort_by_prec_desc)[0]\n",
    "    print(\"Code: {code}\\tF1:{f1}\\tRec:{rec}\\tPrec:{prec}\".format(\n",
    "        code=code.ljust(3), \n",
    "        f1=best_f1.algo.ljust(ALGO_NAME_WIDTH), \n",
    "        rec=best_rec.algo.ljust(ALGO_NAME_WIDTH), \n",
    "        prec=best_prec.algo.ljust(ALGO_NAME_WIDTH)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
