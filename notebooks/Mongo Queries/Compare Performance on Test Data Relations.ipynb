{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "data_directory =  \"/Users/simon.hughes/Google Drive/Phd/Data/\"\n",
    "root_folder = data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays_cb = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "226 files found\n",
      "226 essays processed\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_process_essays\n",
    "from window_based_tagger_config import get_config\n",
    "from Settings import Settings\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test_cb = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    pred_tagged_essays_sc = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 files found\n",
      "218 essays processed\n"
     ]
    }
   ],
   "source": [
    "root_folder = settings.data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test_sc = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_collections = [\n",
    "    'CR_CB_TAGGING_VD_MOST_COMMON_TAG_RNN',\n",
    "    'CR_CB_STACKED_VD',\n",
    "    'CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM_VD'\n",
    "]\n",
    "test_collections = [\"TEST_\" + c.replace(\"HYPER_PARAM_\",\"\") for c in cb_collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics_causal\n",
    "\n",
    "def print_macro_table_row(coll, algo, crels, db=db):\n",
    "    best_f1 = -1\n",
    "    best_tuple = None\n",
    "    for row in db[coll].find({}):\n",
    "        precision, recall,ncodes = [],[],[]        \n",
    "        for k in row.keys():\n",
    "            #TODO - fix this to be more specific\n",
    "            if k in crels:\n",
    "                code, prec, rec = k, row[k][\"precision\"], row[k][\"recall\"]\n",
    "                precision.append(prec)\n",
    "                recall.append(rec)\n",
    "                ncodes.append(row[k][\"num_codes\"])\n",
    "        mprec = np.mean(precision)\n",
    "        mrec =  np.mean(recall)\n",
    "        count = sum(ncodes)\n",
    "        if mprec > 0 or mrec > 0:\n",
    "            est_mf1 = (2 * mprec * mrec) / (mprec + mrec)\n",
    "            #For some reason, the MONGO macro F1 does not match the expected from all the classes\n",
    "            #let's compute this here so that the math works (in case they compute it from the underlying macro recall and prec)\n",
    "            print(\"{algo} &\\t{count}\\t&\\t{mf1:.3f}\\t\\t&\\t{rec:.3f}\\t\\t&\\t{prec:.3f} \\\\\\\\\".format(\n",
    "                algo=algo.ljust(25), count=count, mf1=est_mf1, prec=mprec, rec=mrec))\n",
    "        else:\n",
    "            print(\"{algo} &\\t{count}\\t&\\t{mf1:.3f}\\t\\t&\\t{rec:.3f}\\t\\t&\\t{prec:.3f} \\\\\\\\\".format(\n",
    "                algo=algo.ljust(25), count=count, mf1=0.0, prec=0.0, rec=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_micro_table_row(coll, algo, crels, db=db):\n",
    "    best_f1 = -1\n",
    "    best_tuple = None\n",
    "    for row in db[coll].find({}):\n",
    "        tp,fp,fn, ncodes = 0,0,0,0\n",
    "        for k in row.keys():\n",
    "            #TODO - fix this to be more specific\n",
    "            if k in crels:\n",
    "                metrics = row[k]\n",
    "                tp += metrics[\"tp\"]\n",
    "                fp += metrics[\"fp\"]\n",
    "                fn += metrics[\"fn\"]\n",
    "                ncodes += metrics[\"num_codes\"]\n",
    "        if tp + fp == 0:\n",
    "            mprec = 0\n",
    "        else:\n",
    "            mprec = tp / (tp + fp)\n",
    "        \n",
    "        if tp + fn == 0:\n",
    "            mrec = 0\n",
    "        else:\n",
    "            mrec = tp / (tp + fn)\n",
    "       \n",
    "        if mprec > 0 or mrec > 0:\n",
    "            est_mf1 = (2 * mprec * mrec) / (mprec + mrec)\n",
    "            #For some reason, the MONGO macro F1 does not match the expected from all the classes\n",
    "            #let's compute this here so that the math works (in case they compute it from the underlying macro recall and prec)\n",
    "            print(\"{algo} &\\t{count}\\t&\\t{mf1:.3f}\\t\\t&\\t{rec:.3f}\\t\\t&\\t{prec:.3f} \\\\\\\\\".format(\n",
    "                algo=algo.ljust(25), count=ncodes, mf1=est_mf1, prec=mprec, rec=mrec))\n",
    "        else:\n",
    "            print(\"{algo} &\\t{count}\\t&\\t{mf1:.3f}\\t\\t&\\t{rec:.3f}\\t\\t&\\t{prec:.3f} \\\\\\\\\".format(\n",
    "                algo=algo.ljust(25), count=ncodes, mf1=0.0, prec=0.0, rec=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_code_metrics(coll, crels, db=db):\n",
    "    best_f1 = -1\n",
    "    best_tuple = None\n",
    "    for row in db[coll].find({}):\n",
    "        tp,fp,fn, ncodes = 0,0,0,0\n",
    "        for k in row.keys():\n",
    "            #TODO - fix this to be more specific\n",
    "            if k in crels:\n",
    "                metrics = row[k]\n",
    "                tp = metrics[\"tp\"]\n",
    "                tn = metrics[\"tn\"]\n",
    "                fp = metrics[\"fp\"]\n",
    "                fn = metrics[\"fn\"]\n",
    "                ncodes = metrics[\"num_codes\"]\n",
    "                print(\"{code} \\t tp={tp} \\t tn={tn} \\t fp={fp} \\t fn={fn} \\t codes={codes}\".format(\n",
    "                    code=k.ljust(25), tp=tp, tn=tn, fp=fp, fn=fn, codes=ncodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_causal_algo_name(coll):\n",
    "    if \"RNN\" in coll:\n",
    "        return \"RNN Word Tagging Model\"\n",
    "    if \"SHIFT\" in coll:\n",
    "        return \"Shift-Reduce Parser\"\n",
    "    if \"STACK\" in coll:\n",
    "        return \"Stacking Model\"\n",
    "    return \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Causer:5->Result:11',\n",
       " 'Causer:6->Result:5',\n",
       " 'Causer:7->Result:1',\n",
       " 'Causer:7->Result:13'}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crel_helper import get_tag_freq\n",
    "\n",
    "tag_freq_train = get_tag_freq(pred_tagged_essays_cb, [])\n",
    "crel_tags_train = list((t for t in tag_freq_train.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                ))\n",
    "\n",
    "tag_freq_test = get_tag_freq(tagged_essays_test_cb, [])\n",
    "crel_tags_test = list((t for t in tag_freq_test.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                ))\n",
    "test_crels_cb = set(crel_tags_test) - set(crel_tags_train)\n",
    "test_crels_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Causer:11->Result:4', 'Causer:5->Result:12'}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crel_helper import get_tag_freq\n",
    "\n",
    "tag_freq_train = get_tag_freq(pred_tagged_essays_sc, [])\n",
    "crel_tags_train = list((t for t in tag_freq_train.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                ))\n",
    "\n",
    "tag_freq_test = get_tag_freq(tagged_essays_test_sc, [])\n",
    "crel_tags_test = list((t for t in tag_freq_test.keys() if (\"->\" in t) and\n",
    "                not \"Anaphor\" in t and\n",
    "                not \"other\" in t and\n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and\n",
    "                1 == 1\n",
    "                ))\n",
    "test_crels_sc = set(crel_tags_test) - set(crel_tags_train)\n",
    "test_crels_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coral Bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causer:5->Result:11       \t tp=0.0 \t tn=1917.0 \t fp=0.0 \t fn=1.0 \t codes=1\n",
      "Causer:6->Result:5        \t tp=1.0 \t tn=1917.0 \t fp=0.0 \t fn=0.0 \t codes=1\n",
      "Causer:7->Result:13       \t tp=0.0 \t tn=1916.0 \t fp=1.0 \t fn=1.0 \t codes=1\n",
      "Causer:7->Result:1        \t tp=0.0 \t tn=1917.0 \t fp=0.0 \t fn=1.0 \t codes=1\n"
     ]
    }
   ],
   "source": [
    "for coll in test_collections[-1:]:\n",
    "    print_code_metrics(coll, crels=test_crels_cb, db = client.metrics_causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Word Tagging Model    &\t4\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Stacking Model            &\t4\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Shift-Reduce Parser       &\t4\t&\t0.333\t\t&\t0.250\t\t&\t0.500 \\\\\n"
     ]
    }
   ],
   "source": [
    "for coll in test_collections:\n",
    "    name = get_causal_algo_name(coll)\n",
    "    print_micro_table_row(coll, name, crels=test_crels_cb, db = client.metrics_causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Word Tagging Model    &\t4\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Stacking Model            &\t4\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Shift-Reduce Parser       &\t4\t&\t0.250\t\t&\t0.250\t\t&\t0.250 \\\\\n"
     ]
    }
   ],
   "source": [
    "for coll in test_collections:\n",
    "    name = get_causal_algo_name(coll)\n",
    "    print_macro_table_row(coll, name, crels=test_crels_cb, db = client.metrics_causal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causer:5->Result:12       \t tp=0.0 \t tn=2096.0 \t fp=0.0 \t fn=1.0 \t codes=1\n",
      "Causer:11->Result:4       \t tp=0.0 \t tn=2096.0 \t fp=0.0 \t fn=1.0 \t codes=1\n"
     ]
    }
   ],
   "source": [
    "for coll in test_collections[-1:]:\n",
    "    coll = coll.replace(\"CB\", \"SC\")\n",
    "    print_code_metrics(coll, crels=test_crels_sc, db = client.metrics_causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Word Tagging Model    &\t2\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Stacking Model            &\t2\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Shift-Reduce Parser       &\t2\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n"
     ]
    }
   ],
   "source": [
    "for coll in test_collections:\n",
    "    coll = coll.replace(\"CB\", \"SC\")\n",
    "    #print(test_crels_sc)\n",
    "    name = get_causal_algo_name(coll)\n",
    "    print_micro_table_row(coll, name, crels=test_crels_sc, db = client.metrics_causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Word Tagging Model    &\t2\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Stacking Model            &\t2\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n",
      "Shift-Reduce Parser       &\t2\t&\t0.000\t\t&\t0.000\t\t&\t0.000 \\\\\n"
     ]
    }
   ],
   "source": [
    "for coll in test_collections:\n",
    "    coll = coll.replace(\"CB\", \"SC\")\n",
    "    name = get_causal_algo_name(coll)\n",
    "    print_macro_table_row(coll, name, crels=test_crels_sc, db = client.metrics_causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
