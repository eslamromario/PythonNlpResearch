{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to Train A Keras Model in a Many to 1 Fashion - Make a Prediction Once Per Sentence (Return Sequences = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check mongo is running\n",
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - To Get this working:\n",
    "\n",
    "* Install CUDA and associated libraries, setup path\n",
    "* Install bleeding edge theano (from src)\n",
    "* Make sure the THEANO_FLAGS are set correctly via the environment var, or via the ~/.theanorc file\n",
    "* Install and compile bleeding edge Keras (from src)\n",
    "* `export KERAS_BACKEND=theano`\n",
    "* `export KERAS_IMAGE_DIM_ORDERING='th'`\n",
    "* `sh <project_root>/shell_scipts/setup_environment.sh` to install additional dependencies\n",
    "* **DO NOT SET UNROLL=True** when creating RNN's - causes max recursion issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "* You may need to clean the theano cache. To do so thoroughly, run this command from the shell:\n",
    " * `theano-cache purge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/anaconda/envs/theano_keras/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "# on my new laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaconda-client==1.6.3\r\n",
      "appnope==0.1.0\r\n",
      "bleach==2.1.2\r\n",
      "certifi==2016.2.28\r\n",
      "clyent==1.2.2\r\n",
      "decorator==4.2.1\r\n",
      "dill==0.2.6\r\n",
      "entrypoints==0.2.3\r\n",
      "html5lib==1.0.1\r\n",
      "ipykernel==4.8.0\r\n",
      "ipython==6.2.1\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.1.1\r\n",
      "jedi==0.11.1\r\n",
      "Jinja2==2.10\r\n",
      "joblib==0.11\r\n",
      "jsonschema==2.6.0\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client==5.2.2\r\n",
      "jupyter-console==5.2.0\r\n",
      "jupyter-core==4.4.0\r\n",
      "Keras==2.1.3\r\n",
      "Mako==1.0.6\r\n",
      "MarkupSafe==1.0\r\n",
      "mistune==0.8.3\r\n",
      "nb-anacondacloud==1.4.0\r\n",
      "nb-conda==2.2.0\r\n",
      "nb-conda-kernels==2.1.0\r\n",
      "nbconvert==5.3.1\r\n",
      "nbformat==4.4.0\r\n",
      "nbpresent==3.0.2\r\n",
      "nltk==3.2.5\r\n",
      "nose==1.3.7\r\n",
      "notebook==5.4.0\r\n",
      "numpy==1.13.1\r\n",
      "pandocfilters==1.4.2\r\n",
      "parso==0.1.1\r\n",
      "pexpect==4.3.1\r\n",
      "pickleshare==0.7.4\r\n",
      "prompt-toolkit==1.0.15\r\n",
      "ptyprocess==0.5.2\r\n",
      "Pygments==2.2.0\r\n",
      "pygpu==0.6.9\r\n",
      "pymongo==3.4.0\r\n",
      "python-dateutil==2.6.1\r\n",
      "pytz==2017.2\r\n",
      "PyYAML==3.12\r\n",
      "pyzmq==16.0.4\r\n",
      "qtconsole==4.3.1\r\n",
      "requests==2.14.2\r\n",
      "scipy==0.19.1\r\n",
      "Send2Trash==1.4.2\r\n",
      "simplegeneric==0.8.1\r\n",
      "six==1.10.0\r\n",
      "terminado==0.8.1\r\n",
      "testpath==0.3.1\r\n",
      "Theano==0.9.0\r\n",
      "tornado==4.5.3\r\n",
      "traitlets==4.3.2\r\n",
      "wcwidth==0.1.7\r\n",
      "webencodings==0.5.1\r\n",
      "widgetsnbextension==3.1.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "WARNING - No db name specified - should be either 'metrics_causal' or 'metrics'. Defaulting to 'metrics' \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "models_folder = root_folder + \"Models/Bi-LSTM/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/training.pl\n"
     ]
    }
   ],
   "source": [
    "print(training_pickled)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = get_config(training_folder)\n",
    "tagged_essays_tmp = load_process_essays(**config)\n",
    "\n",
    "with open(training_pickled, \"wb+\") as f:\n",
    "    pickle.dump(tagged_essays_tmp, f)\n",
    "del tagged_essays_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 files found\n",
      "226 essays processed\n"
     ]
    }
   ],
   "source": [
    "# load the test essays to make sure we compute metrics over the test CR labels\n",
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2018-01-28 14:36:42.061472\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1641, 86, 87)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "for essay in tagged_essays_test:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            # don't add unique words from the test data - impacts the embedding matrix size, causes out of bounds index error\n",
    "            #unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "                \n",
    "EMPTY_TAG = \"Empty\"\n",
    "#regular_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and not \"Anaphor\" in t and not \"other\" in t and not \"rhetorical\" in t))\n",
    "#regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "regular_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and \n",
    "                not \"Anaphor\" in t and \n",
    "                not \"other\" in t and \n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and \n",
    "                1==1\n",
    "               ))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "len(unique_words), len(regular_tags), len(vtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:1->Result:11',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:12->Result:11',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:11',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5->Result:50',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:6->Result:5',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:7->Result:1',\n",
       " 'Causer:7->Result:13',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Essays into Training Data (Word Ids)\n",
    "\n",
    "* Computes `xs`, `ys`, `ys_bytag` and `seq_lens`\n",
    "* `ys_bytag` includes **all tags** and does **not** focus only on the most common tag\n",
    "* `ys` only includes the most common tag (so we can use cross entropy)\n",
    "* `seq_lens` is without the start and end tags included (so we have to map back and forth to maintain mappings)\n",
    "* `ys_bytag` also excludes the START and END tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Sequence Length, Generate All Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix2tag = {}\n",
    "for ix, t in enumerate(vtags):\n",
    "    ix2tag[ix] = t\n",
    "    \n",
    "generator = idGen(seed=1) # important as we zero pad sequences\n",
    "\n",
    "maxlen = 0\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "\n",
    "def ids2tags(ids):\n",
    "    return [generator.get_key(j) for j in ids]  \n",
    "\n",
    "def lbls2tags(ixs):\n",
    "    return [ix2tag[ix] for ix in ixs]\n",
    "        \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END   = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays):\n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag = defaultdict(list)\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_sent = []\n",
    "            unique_tags = set()\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)                \n",
    "                # remove unwanted tags\n",
    "                tags = vtags.intersection(tags)\n",
    "                unique_tags.update(tags)\n",
    "                # retain all tags for evaluation (not just most common)\n",
    "                # SKIP the START and END tags\n",
    "                if word != START and word != END:\n",
    "                    for t in (vtags - set([EMPTY_TAG])):\n",
    "                        if t in tags:\n",
    "                            ys_bytag[t].append(1)\n",
    "                        else:\n",
    "                            ys_bytag[t].append(0)\n",
    "\n",
    "                # encode ys with most common tag only\n",
    "            \n",
    "            if len(unique_tags) == 0:\n",
    "                unique_tags.add(EMPTY_TAG)\n",
    "                \n",
    "            for tag in vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_sent[tag].append(1)\n",
    "                    y_sent.append(1)\n",
    "                else:\n",
    "                    ys_bytag_sent[tag].append(0)\n",
    "                    y_sent.append(0)\n",
    "                \n",
    "            seq_lens.append(len(row)-2)\n",
    "            ys.append(y_sent)\n",
    "            xs.append(row)\n",
    "    \n",
    "    xs = sequence.pad_sequences(xs, maxlen=maxlen)\n",
    "    #ys = sequence.pad_sequences(ys, maxlen=maxlen)\n",
    "    ys = np.array(ys)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert ys.shape[1] == len(vtags)\n",
    "    #assert xs.shape[1] == ys.shape[1] == maxlen, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag, ys_bytag_sent, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_level_preds(preds):\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for row in preds:\n",
    "        pred_ys = set()\n",
    "        for ix, v in enumerate(row):\n",
    "            if v >= 0.5:\n",
    "                tag = ix2tag[ix]\n",
    "                pred_ys.add(tag)\n",
    "\n",
    "        for tag in vtags:\n",
    "            if tag == EMPTY_TAG:\n",
    "                continue\n",
    "            if tag in pred_ys:\n",
    "                pred_ys_by_tag[tag].append(1)\n",
    "            else:\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "\n",
    "    if EMPTY_TAG in pred_ys_by_tag:\n",
    "        del pred_ys_by_tag[EMPTY_TAG]\n",
    "    \n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dev_split(lst, dev_split):\n",
    "    # random shuffle\n",
    "    shuffle(lst)\n",
    "    num_training = int((1.0 - dev_split) * len(lst))\n",
    "    return lst[:num_training], lst[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 603 ms, total: 13 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this name for a different function later\n",
    "from CrossValidation import cross_validation as cv\n",
    "\n",
    "folds = cv(tagged_essays, CV_FOLDS)\n",
    "fold2training_data = {}\n",
    "fold2dev_data = {}\n",
    "fold2test_data = {}\n",
    "\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    # further split into train and dev test\n",
    "    essays_train, essays_dev = train_dev_split(essays_TD, DEV_SPLIT)\n",
    "    fold2training_data[i] = get_training_data(essays_train)\n",
    "    fold2dev_data[i]      = get_training_data(essays_dev)\n",
    "    # Test Data\n",
    "    fold2test_data[i]     = get_training_data(essays_VD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs, ys, ys_bytag, ys_bytag_sent, seq_lens = fold2training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5963, 87), 87, True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape, len(vtags), EMPTY_TAG in vtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove 100 Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/DeepLearning/WordVectors/pickle_glove_embedding.py\n",
    "# for creating pre-filtered embeddings file\n",
    "import pickle, os\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings_file = \"/Users/simon.hughes/data/word_embeddings/glove.6B/cb_dict_glove.6B.100d.txt\"\n",
    "# read data file\n",
    "with open(embeddings_file, \"rb+\") as f:\n",
    "    cb_emb_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1641 2.5 %\n"
     ]
    }
   ],
   "source": [
    "missed = set()\n",
    "for wd in unique_words:\n",
    "    if wd not in cb_emb_index:\n",
    "        missed.add(wd)\n",
    "print(len(missed), len(unique_words), 100.0 * round(len(missed)/  len(unique_words),4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = list(cb_emb_index.values())[0].shape[0]\n",
    "\n",
    "def get_embedding_matrix(words, idgenerator, max_features, init='uniform', unit_length=False):\n",
    "    embedding_dim = list(cb_emb_index.values())[0].shape[0]\n",
    "    # initialize with a uniform distribution\n",
    "    if init == 'uniform':\n",
    "        # NOTE: the max norms for these is quite low relative to the embeddings\n",
    "        embedding_matrix = np.random.uniform(low=-0.05, high=0.05,size=(max_features, embedding_dim))\n",
    "    elif init =='zeros':\n",
    "        embedding_matrix = np.zeros(shape=(max_features, embedding_dim), dtype=np.float32)\n",
    "    elif init == 'normal':\n",
    "        embedding_matrix = np.random.normal(mean, sd, size=(max_features, embedding_dim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown init type\")\n",
    "    for word in words:\n",
    "        i = idgenerator.get_id(word)\n",
    "        embedding_vector = cb_emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    if unit_length:\n",
    "        norms = np.linalg.norm(embedding_matrix, axis=1,keepdims=True)\n",
    "        # remove 0 norms to prevent divide by zero\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embedding_matrix = embedding_matrix / norms\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict(xs, batch_size=batch_size, verbose=0)   \n",
    "    pred_ys_by_tag = get_sentence_level_preds(preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-01-28 14:41:54.444371', '20180128_144154_444405')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from datetime import datetime\n",
    "\n",
    "def get_ts():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_file_ts():\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "\n",
    "embedding_size = EMBEDDING_DIM\n",
    "hidden_size    = 128\n",
    "out_size = len(vtags)\n",
    "batch_size = 128\n",
    "\n",
    "get_ts(), get_file_ts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bi-Directional LSTM With Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "\n",
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_fold(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "\n",
    "    if use_pretrained_embedding:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, \n",
    "                                                init='uniform', \n",
    "                                                unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    else:\n",
    "        embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "    if bi_directional:\n",
    "        rnn_layer_fact = lambda return_sequences: Bidirectional(GRU(units=hidden_size, return_sequences=return_sequences, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "    else:\n",
    "        rnn_layer_fact = lambda return_sequences: GRU(units=hidden_size, return_sequences=return_sequences, consume_less=\"cpu\")\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        # Don't return sequences in the final layer\n",
    "        if i == (num_rnns-1):\n",
    "            model.add(rnn_layer_fact(return_sequences=False))\n",
    "        else:\n",
    "            model.add(rnn_layer_fact(return_sequences=True))\n",
    "\n",
    "    model.add(Dense(out_size))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='kullback_leibler_divergence', optimizer='adam')\n",
    "    \n",
    "    X_train, y_train, train_ys_by_tag, train_ys_by_tag_sent, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev,   y_dev,   dev_ys_by_tag,   dev_ys_by_tag_sent,   seq_len_dev   = fold2dev_data[fold_ix]\n",
    "    X_test,  y_test,  test_ys_by_tag,  test_ys_by_tag_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 10\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "        #print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1 # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_by_tag_sent, seq_len_dev)\n",
    "\n",
    "        #print(micro_metrics)\n",
    "        #print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Fold[{ix}] - Best F1 Score={f1}\".format(ix=fold_ix, f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    train_micro_metrics, train_predictions_by_tag = score_predictions(model, X_train, train_ys_by_tag_sent, seq_len_train)\n",
    "    test_micro_metrics,  test_predictions_by_tag  = score_predictions(model, X_test,  test_ys_by_tag_sent,  seq_len_test)\n",
    "    return train_predictions_by_tag, test_predictions_by_tag, train_ys_by_tag_sent, test_ys_by_tag_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    \n",
    "    results = [evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) \n",
    "                for i in range(CV_FOLDS)]\n",
    "\n",
    "    cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    for result in results:\n",
    "        td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = result\n",
    "        merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "        merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "        merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "        merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "        \n",
    "    SUFFIX = \"_BINARY_CROSSENT_TAG_RNN\"\n",
    "    CB_TAGGING_TD, CB_TAGGING_VD = \"CR_CB_TAGGING_TD\" + SUFFIX, \"CR_CB_TAGGING_VD\" + SUFFIX\n",
    "    parameters = dict(config)\n",
    "    parameters[\"extractors\"] = []\n",
    "    parameters[\"min_feat_freq\"] = 0\n",
    "\n",
    "    parameters[\"loss\"] = \"bce\"\n",
    "    parameters[\"use_pretrained_embedding\"] = use_pretrained_embedding\n",
    "    parameters[\"bi-directional\"] = bi_directional\n",
    "    parameters[\"hidden_size\"] = hidden_size\n",
    "    parameters[\"merge_mode\"] = merge_mode\n",
    "    parameters[\"num_rnns\"] = num_rnns\n",
    "\n",
    "    wd_algo = \"RNN\"\n",
    "    wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag, parameters, wd_algo)\n",
    "    wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag, parameters, wd_algo)\n",
    "    avg_f1 = float(processor.get_metric(CB_TAGGING_VD, wd_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Params 2018-01-28 15:33:48.178454 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=64\n",
      "Fold[0] - Best F1 Score=0.010204339370058851\n",
      "Fold[1] - Best F1 Score=0.010601712921385231\n",
      "Fold[2] - Best F1 Score=0.009653823254231522\n",
      "Fold[3] - Best F1 Score=0.0118503937007874\n",
      "Fold[4] - Best F1 Score=0.009464519537800647\n",
      "MicroF1=0.010781553185591526\n",
      "[2] Params 2018-01-28 15:46:34.882781 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=128\n",
      "Fold[0] - Best F1 Score=0.009010202729561415\n",
      "Fold[1] - Best F1 Score=0.009737255059751338\n",
      "Fold[2] - Best F1 Score=0.008666001017313163\n",
      "Fold[3] - Best F1 Score=0.010119166500233007\n",
      "Fold[4] - Best F1 Score=0.008578123692359194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /Users/simon.hughes/.theano/compiledir_Darwin-17.3.0-x86_64-i386-64bit-i386-3.5.4-64/lock_dir/lock\n",
      "2018-01-28 16:10:16,105 : INFO : Refreshing lock /Users/simon.hughes/.theano/compiledir_Darwin-17.3.0-x86_64-i386-64bit-i386-3.5.4-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MicroF1=0.009643376119147258\n",
      "[3] Params 2018-01-28 16:10:15.911294 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /Users/simon.hughes/.theano/compiledir_Darwin-17.3.0-x86_64-i386-64bit-i386-3.5.4-64/lock_dir/lock\n",
      "2018-01-28 16:11:30,284 : INFO : Refreshing lock /Users/simon.hughes/.theano/compiledir_Darwin-17.3.0-x86_64-i386-64bit-i386-3.5.4-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold[0] - Best F1 Score=0.008034791426955554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-2da7ff4a5414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\u001b[0m                          \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pretrained_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_pretrained_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_directional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbi_directional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rnns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mmicro_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_pretrained_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_directional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MicroF1={micro_f1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmicro_f1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-98975d557d63>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     results = [evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) \n\u001b[0;32m----> 4\u001b[0;31m                 for i in range(CV_FOLDS)]\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcv_wd_td_ys_by_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_wd_td_predictions_by_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-98975d557d63>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     results = [evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) \n\u001b[0;32m----> 4\u001b[0;31m                 for i in range(CV_FOLDS)]\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcv_wd_td_ys_by_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_wd_td_predictions_by_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-a2e43107bd46>\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# epochs per training instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mmicro_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ys_by_tag_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/simon.hughes/.theano/compiledir_Darwin-17.3.0-x86_64-i386-64bit-i386-3.5.4-64/scan_perform/mod.cpp:7110)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/theano_keras/lib/python3.5/site-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mvalue_zeros\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i = 0\n",
    "for use_pretrained_embedding in [True]:\n",
    "    for bi_directional in [True]:\n",
    "        for num_rnns in [1,2]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [64, 128, 256]:\n",
    "                    i += 1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    micro_f1 = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    print(\"MicroF1={micro_f1}\".format(micro_f1=micro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Use early stopping criteria\n",
    "* Embeddings:\n",
    " * Don't remove low frequency words\n",
    " * Normalize all vector lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
